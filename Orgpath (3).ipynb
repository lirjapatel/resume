{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:30.148068Z",
     "iopub.status.busy": "2025-02-02T03:33:30.147540Z",
     "iopub.status.idle": "2025-02-02T03:33:30.151019Z",
     "shell.execute_reply": "2025-02-02T03:33:30.150538Z"
    }
   },
   "outputs": [],
   "source": [
    "# result = resumes_collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:30.152957Z",
     "iopub.status.busy": "2025-02-02T03:33:30.152596Z",
     "iopub.status.idle": "2025-02-02T03:33:31.091178Z",
     "shell.execute_reply": "2025-02-02T03:33:31.090501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:31.093226Z",
     "iopub.status.busy": "2025-02-02T03:33:31.093019Z",
     "iopub.status.idle": "2025-02-02T03:33:31.100619Z",
     "shell.execute_reply": "2025-02-02T03:33:31.100096Z"
    }
   },
   "outputs": [],
   "source": [
    "# # API keys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "uri = os.environ.get('MONGODB_URI')\n",
    "api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:31.102671Z",
     "iopub.status.busy": "2025-02-02T03:33:31.102297Z",
     "iopub.status.idle": "2025-02-02T03:33:31.956634Z",
     "shell.execute_reply": "2025-02-02T03:33:31.955874Z"
    },
    "id": "75Ye16UW_ehX",
    "outputId": "b6e67e6d-68c7-4195-f0ec-9c363c91d5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (4.11)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pymongo) (2.7.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymongo  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:31.958903Z",
     "iopub.status.busy": "2025-02-02T03:33:31.958695Z",
     "iopub.status.idle": "2025-02-02T03:33:32.389165Z",
     "shell.execute_reply": "2025-02-02T03:33:32.388544Z"
    },
    "id": "-C9kUPdf_fbk"
   },
   "outputs": [],
   "source": [
    "import certifi\n",
    "import pymongo  #this is a library to allows us to work with mongodb from python\n",
    "from pymongo.mongo_client import MongoClient   #this is used to create a connection to a mongodb instance\n",
    "from pymongo.server_api import ServerApi    #is used to specify the version of mongodb server API to use when connecting\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime, timezone\n",
    "from pymongo.errors import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:32.391538Z",
     "iopub.status.busy": "2025-02-02T03:33:32.391128Z",
     "iopub.status.idle": "2025-02-02T03:33:32.894617Z",
     "shell.execute_reply": "2025-02-02T03:33:32.894015Z"
    },
    "id": "-jdYfFpxEYXO",
    "outputId": "d737fede-73a9-4b8c-d61f-8a9421ea5190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting MongoDB Connection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# #URI is (Uniform Resource Identifier)\n",
    "# # we used username , password , cluster address and connection options like (retryWrites=true, w=majority, appName=Cluster0)\n",
    "\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "# client = MongoClient(uri, server_api=ServerApi('1'), tlsCAFile=certifi.where()),\n",
    "# server_api=ServerApi('1'), tlsCAFile=certifi.where()\n",
    "\n",
    "\n",
    "# # Send a ping to confirm a successful connection and printing a success message or an error it it fails\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import certifi\n",
    "from pymongo.errors import ConfigurationError, ConnectionFailure\n",
    "import sys\n",
    "import os\n",
    "import pymongo\n",
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "import os\n",
    "import pymongo\n",
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Get MongoDB URI from environment variables\n",
    "uri = os.environ.get('MONGODB_URI')\n",
    "if not uri:\n",
    "    raise ValueError(\"MongoDB URI is not set in environment variables\")\n",
    "\n",
    "print(\"Attempting MongoDB Connection...\")\n",
    "\n",
    "client = MongoClient(\n",
    "    uri, \n",
    "    server_api=ServerApi('1'),  # Ensure this line has a comma at the end\n",
    "    tlsCAFile=certifi.where(),\n",
    "    connectTimeoutMS=30000,\n",
    "    socketTimeoutMS=30000\n",
    ")\n",
    "\n",
    "# Ping the deployment\n",
    "client.admin.command('ping')\n",
    "print(\"Successfully connected to MongoDB!\")\n",
    "\n",
    "\n",
    "\n",
    "# # Explicitly load environment variables\n",
    "# # load_dotenv()\n",
    "\n",
    "# # # Print environment details\n",
    "# # print(\"Python Version:\", sys.version)\n",
    "# # print(\"Current Working Directory:\", os.getcwd())\n",
    "# # print(\"Environment Variables:\")\n",
    "# # print(\"MONGODB_URI:\", os.getenv('MONGODB_URI'))\n",
    "\n",
    "# try:\n",
    "#     import pymongo\n",
    "\n",
    "#     # Detailed connection attempt\n",
    "#     uri = os.getenv('MONGODB_URI')\n",
    "#     if not uri:\n",
    "#         raise ValueError(\"MongoDB URI is not set in environment variables\")\n",
    "\n",
    "#     print(\"Attempting MongoDB Connection...\")\n",
    "#     client = MongoClient(\n",
    "#         uri, \n",
    "#         server_api=ServerApi('1'), \n",
    "#         tlsCAFile=certifi.where(),\n",
    "#         connectTimeoutMS=30000,\n",
    "#         socketTimeoutMS=30000\n",
    "#     )\n",
    "\n",
    "#     # Ping the deployment\n",
    "#     client.admin.command('ping')\n",
    "#     print(\"Successfully connected to MongoDB!\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Full Error Details:\")\n",
    "#     print(f\"Error Type: {type(e).__name__}\")\n",
    "#     print(f\"Error Message: {str(e)}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0MdDcrrLjgU"
   },
   "source": [
    "\n",
    "NEW CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:32.896823Z",
     "iopub.status.busy": "2025-02-02T03:33:32.896412Z",
     "iopub.status.idle": "2025-02-02T03:33:42.693249Z",
     "shell.execute_reply": "2025-02-02T03:33:42.692503Z"
    },
    "id": "eAJRdz82nB5v",
    "outputId": "52e7fde3-4f63-49e5-d3c8-ca6494968d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (4.11)\r\n",
      "Requirement already satisfied: python-dotenv in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: openai in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.61.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\r\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting python-docx\r\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\r\n",
      "  Using cached langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pymongo) (2.7.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.8.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.8.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (2.10.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.12.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml>=3.1.0 (from python-docx)\r\n",
      "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from langchain_community) (6.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\r\n",
      "  Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\r\n",
      "  Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\r\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\r\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain<0.4.0,>=0.3.16 (from langchain_community)\r\n",
      "  Using cached langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-core<0.4.0,>=0.3.32 (from langchain_community)\r\n",
      "  Using cached langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\r\n",
      "  Using cached langsmith-0.3.4-py3-none-any.whl.metadata (14 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2,>=1.22.4 (from langchain_community)\r\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\r\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from langchain_community) (2.32.3)\r\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\r\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\r\n",
      "  Using cached marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\r\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain<0.4.0,>=0.3.16->langchain_community)\r\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.32->langchain_community)\r\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.32->langchain_community) (24.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain_community)\r\n",
      "  Using cached orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain_community)\r\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\r\n",
      "  Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\r\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain_community) (3.0.0)\r\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\r\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\r\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\r\n",
      "Using cached langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\r\n",
      "Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\r\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\r\n",
      "Using cached langchain-0.3.17-py3-none-any.whl (1.0 MB)\r\n",
      "Using cached langchain_core-0.3.33-py3-none-any.whl (412 kB)\r\n",
      "Using cached langsmith-0.3.4-py3-none-any.whl (333 kB)\r\n",
      "Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\r\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\r\n",
      "Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\r\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\r\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Using cached frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\r\n",
      "Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\r\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\r\n",
      "Using cached marshmallow-3.26.0-py3-none-any.whl (50 kB)\r\n",
      "Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\r\n",
      "Using cached orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\r\n",
      "Using cached propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\r\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\r\n",
      "Using cached yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\r\n",
      "Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: zstandard, tenacity, PyPDF2, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, lxml, jsonpatch, httpx-sse, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, python-docx, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain_community\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 2.2.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling numpy-2.2.2:\r\n",
      "      Successfully uninstalled numpy-2.2.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed PyPDF2-3.0.1 SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.17 langchain-core-0.3.33 langchain-text-splitters-0.3.5 langchain_community-0.3.16 langsmith-0.3.4 lxml-5.3.0 marshmallow-3.26.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.15 propcache-0.2.1 pydantic-settings-2.7.1 python-docx-1.1.2 requests-toolbelt-1.0.0 tenacity-9.0.0 typing-inspect-0.9.0 yarl-1.18.3 zstandard-0.23.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymongo python-dotenv openai PyPDF2 python-docx langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:42.695787Z",
     "iopub.status.busy": "2025-02-02T03:33:42.695353Z",
     "iopub.status.idle": "2025-02-02T03:33:44.510614Z",
     "shell.execute_reply": "2025-02-02T03:33:44.509823Z"
    },
    "id": "m0T0gq1nKPO5",
    "outputId": "17549376-15a4-43e1-fd70-c67e18526522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (3.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.1.2)\r\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from python-docx) (5.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from python-docx) (4.12.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 \n",
    "%pip install python-docx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:44.513015Z",
     "iopub.status.busy": "2025-02-02T03:33:44.512673Z",
     "iopub.status.idle": "2025-02-02T03:33:45.513105Z",
     "shell.execute_reply": "2025-02-02T03:33:45.512339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.61.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.8.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.8.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (2.10.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.12.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Requirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7mPU241m5NE"
   },
   "source": [
    "\n",
    "## **Resume Extraction and Processing System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:45.515634Z",
     "iopub.status.busy": "2025-02-02T03:33:45.515135Z",
     "iopub.status.idle": "2025-02-02T03:33:46.009948Z",
     "shell.execute_reply": "2025-02-02T03:33:46.009201Z"
    },
    "id": "Neyqs2toK66I",
    "outputId": "3d891998-b504-4da1-f39d-b066fe78d3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the latest resume in the database.\n",
      "Processing: Resume-1 (7).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from PDF (Resume Text): KAXIT  PANDYA  \n",
      "Linkedin  | kaxitpandya@gmail.com  | +1 (365)  998 6883  | GitHub | Waterloo,  Ontario,  Canada  \n",
      "EDUCATION  \n",
      "University of Waterloo  | Waterloo, Canada          Sep’24 – Sep’25  \n",
      "Master of Engineering  in Computer  Engineering  (Specialization  AI)  \n",
      "Pandit Deendayal Energy University (PDEU)  | Gandhinagar, India          July’20  – July’24  \n",
      "Bachelor of Technology (B.  Tech.)  in Computer  Engineering  – CGPA:  9.63/10   \n",
      "Coursework:  Machine  Learning,  Artificial  Intelligence,  Computer Vision, Data  Mining,  Big Data  Analytics,  Data  Structures,  Design  and Analysis  of \n",
      "Algorithms,  Web Development, Operating Systems, Software Engineering, and Database Management System  \n",
      "WORK  EXPERIENCE  \n",
      " \n",
      "AI/ML  Data  Engineer  | Genellipse  Inc. | Mississauga,  Ontario  Oct’24 - Dec’24  \n",
      "● Built a  text extraction pipeline for processing diverse document formats (PDF, Word, Excel), leveraging Google Cloud Vision API  to extract data \n",
      "from U.S. death certificates with high accuracy  \n",
      "● Utilized BERT -based AI models to map key -value pairs from U.S. death certificate fields accurately and implemented large language  models (LLMs) \n",
      "to interpret extracted information, delivering actionable insights for enhanced client reporting and decision -making  \n",
      "Research  Assistantships  (RA)  | Dr. David  Hammond,  University  of Waterloo  Sep’24  \n",
      "● Implemented an automated web scraping system using Selenium to extract detailed information on grocery items from Walmart, Sobeys, and Target  \n",
      "websites , with integrated data export to Excel sheets for efficient analysis  \n",
      "● Leveraged  an image download module to systematically retrieve product images from scraped websites  and utilized  advanced OCR (Optical \n",
      "Character Recognition) techniques to extract textual information from these downloaded product images  \n",
      "Research  Intern  | Indian  Institute  of Technology  (IIT)  – Gandhinagar  through  SRIP  | Gandhinagar,  Gujarat     May’24  - July’24  \n",
      "● Designed  a Deep Learning model using Generative Adversarial Networks (GANs) to perform inverse design of 2D heterostructure materials, \n",
      "enabling efficient exploration of material properties  \n",
      "● Conducted post -processing of atomistic simulation data using Python to extract and visualize material properties  \n",
      "● Converted QuantumATK software functionalities into Python scripts to automate the creation of various heterostructure configu rations  \n",
      "Research  Intern  | TEEP@ASIA  PLUS  PROGRAM  at National  Chung  Cheng  University  | Chiayi  County,  Taiwan          Jan’24  -Apr’24  \n",
      "● Implemented Federated Learning (FL) model to predict water quality parameters using drone -captured images,  ensuring  privacy across  \n",
      "geographically distributed datasets , and authored a Research paper detailing the methodologies and results  \n",
      "● Executed  robust aggregation strategies such as FedAvg, FedAvgM, FedMedian, and FedProx using the Flower Framework, while mastering  key \n",
      "Federated Learning (FL) concepts, including handling non -IID data distributions and managing system heterogeneity  \n",
      "● Applied Explainable AI (XAI) techniques , SHAP to interpret predictions, highlighting parameter significanc e and enhancing transparency and trust  \n",
      "Research  Intern  | Indian  Space  Research  Organisation  (ISRO),  SAC  | Ahmedabad,  Gujarat  June ’23 - Aug’23  \n",
      "● Developed Deep  Neural  Networks for predicting  classification  of Pulse Radar I nterval  Modulation  \n",
      "● Processed and handled noisy, raw radar pulse data directly from ISRO’s satellites  \n",
      "● Achieved 94% classification accuracy in identifying complex radar Pulse Repetition Intervals (PRI) patterns, including stagge red, jittered, dwell \n",
      "and switch, sliding, linear, and sinusoidal  \n",
      "Software Developer Intern | HumanAnsys | Ahmedabad, Gujarat  Jan’23  - March’23  \n",
      "● Understanding Cloud systems, architecture, and AWS services  \n",
      "● Using AWS services (EC2, S3 bucket, Lambda Functions) to deploy a video calling website  \n",
      "Web Developer Intern | Hybrid Utopia | Ahmedabad, Gujarat  June’22  - Jan’23  \n",
      "● Create websites from scratch using several modern -day frameworks like node, express  \n",
      "● Performing  Create, Read, Update,  Delete ( CRUD ) operations  using  MongoDB  \n",
      "SKILLS  \n",
      " \n",
      "Languages:  Python,  C, C++,  JavaScript,  SQL  \n",
      "Libraries:  NumPy,  Pandas,  Matplotlib,  TensorFlow,  LangChain,  AWS  SageMaker  \n",
      "Framework:  Tailwind  CSS,  Node.js,  Express  \n",
      "Database : MongoDB,  MySQL  \n",
      "Area  of Interest:  Artificial Intelligence (AI), Machine Learning (ML), Generative AI (GenAI)  \n",
      "Add-on skills:  Tableau  \n",
      "RESEARCH  EXPERIENCE  \n",
      " \n",
      "An Intelligent  Fashion  Object  Classification  Using  CNN  (Published ) \n",
      "Federated  Ensemble  based  Water  Quality  Classification  on Drone  Images  (Submitted ) \n",
      "Water Quality Prediction using Machine Learning (Submitted ) \n",
      "Advancement  in phishing  attacks  and Machine  learning  approaches  (Submitted ) PROJECTS  \n",
      " \n",
      "AI-Powered  Instagram  Caption  Generator  (LangChain,  Google’s  PaLM  API, Streamlit ) (Website ) \n",
      "• Developed  Instagram  Caption  Generator  using  LangChain  and Google’s  PaLM  API to create  contextually  relevant  captions  from  user inputs  \n",
      "• Integrated  natural  language  processing  (NLP)  techniques  and prompt  engineering  to ensure  the captions  were  aligned  with the content  and text \n",
      "description  \n",
      "Q&A  Application  on Private  Documents  (LangChain,  Chroma)  (Website ) \n",
      "• Designed  and implemented  a Retrieval -Augmented  Generation  (RAG)  system  to process  and query  custom  PDF and other  document  formats  \n",
      "• Utilized  document  loaders,  chunking  strategies,  and uploaded  embeddings  to Chroma  vector  database  for real-time information  \n",
      "retrieval and Integrated chat history memory to maintain context across interactions  \n",
      "Custom  ChatGPT  (LangChain,  Streamlit)  (Website ) \n",
      "• Built  a custom  conversational  app using  LangChain  and Streamlit,  replicating  a ChatGPT -like user experience  \n",
      "• Designed  and developed  core app functionalities,  including  question -answering,  conversation  display,  and session  history  management  \n",
      "• Incorporated  real-time chat interactions  and tested  the app for performance  and accuracy  in retrieving  information  from  private  documents  \n",
      "Summarizing  Large  Documents  (LangChain,  OpenAI)  (Website ) \n",
      "• Implemented  various  summarization  techniques  using  LangChain  and OpenAI  models  to process  and summarize  large  documents  \n",
      "• Leveraged  LangChain  Agents  for dynamic  summarization  tasks,  reducing  the complexity  of document  review  processes  \n",
      "DeepFake  Audio  Detection  System  (Python,  TensorFlow,  Librosa)  \n",
      "• Preprocessed  audio  data using  Librosa  for feature  extraction,  including  Mel-frequency  cepstral  coefficients  (MFCCs)  and spectrograms  \n",
      "• Utilized  pre-trained models  to classify  audio as  either  Deepfake  or human -generated  \n",
      "Water  Quality  Index  Prediction  (Python,  Machine  Learning  Algorithms)  \n",
      "● Developed  and deployed  predictive  models  to assess  water  quality,  using  algorithms  like Linear  Regressor,  ElasticNet,  Random Forest \n",
      "Regressor, Logistic Regression, Random Forest Classifier, XGBoost, Decision Tree, and K -Nearest Neighbors (KNN)  \n",
      "● Applied  10-fold Cross  Validation  for model  evaluation,  feature  engineering  with SMOTE,  and hyperparameter  tuning  to optimize  performance  \n",
      "Management  System  (Node,  Express,  MongoDB,  Tailwind  CSS)  \n",
      "● Developed  an Inventory  Management  System  featuring  CRUD  operations,  allowing  users  to create,  read,  update,  and delete  inventory  items  \n",
      "Chrome  Extension  for Tracking  Links  (HTML,  CSS,  JavaScript,  Chrome  Extensions  API)  \n",
      "● Developed  a Chrome  extension  that saves  current  tab URLs  and user notes,  allowing for  easy access  and management  of important  links \n",
      "CERTIFICATIONS  \n",
      " \n",
      "‘LangChain  Mastery’  and ‘OpenAI  Python  API Bootcamp’  through  Udemy \n",
      "‘GenAI with LLMs’ by DeepLearning.AI through Coursera  \n",
      "‘Machine  Learning  Specialization’  and ‘Deep  Learning  Specialization’  from  DeepLearning.AI  by Andrew  Ng through  Coursera \n",
      "‘Data  Analyst with Python’  from IIT Madras through National Programme on Technology Enhanced Learning (NPTEL)  \n",
      "‘The  Web  Developer  Bootcamp  2023’  from  Colt Steele  through  Udemy  \n",
      "EXTRACURRICULAR  \n",
      " \n",
      "Engineering  Representative  | Graduate  Student  Endowment  Fund  (GSEF)  | University  of Waterloo  Sep’24  – Sep’25  \n",
      "● Reviewing  and evaluating  funding  applications,  ensuring  strategic  allocation  of university  resources across  diverse  academic  disciplines  to \n",
      "graduate student projects and initiatives  \n",
      "● Providing  strategic  input  on funding  decisions  to enhance  the academic  and professional  development  of graduate  students  \n",
      "Technical  Project Manager  | Waterloo  AI Institute  (WAT.AI)  | University  of Waterloo  Sep’24  – Sep’25  \n",
      "● Building  deployable  AI models  on onboard  satellites,  focusing  on the semantic  segmentation  of methane  plumes  using  hyperspectral  ML models  \n",
      "● Leading  AI research  projects,  managing  teams,  ensuring  timely  completion  and quality  deliverables  in collaboration  with industry  partners  \n",
      "Web  Development  Head  | SnT - Science  & Technical  Club  of PDEU  | Gandhinagar , Gujarat  June’22  – July’23  \n",
      "● Creating  a Website  for the university  flagship  event  named  Tesseract  (Github ) \n",
      "Chairperson  | GeeksForGeeks  - GeeksForGeeks  Student  Chapter  of PDEU  | Gandhinagar,  Gujarat  June’22  – July’23  \n",
      "● Management  of Events  and Workshop  at regular  intervals  \n",
      "● Leading  a team  of 50 members  and Developing Coding Culture at  our University  \n",
      "Open  Source  Contributor  | Hacktoberfest  2022  October‘22  \n",
      "● Made  more  than 6 contributions  to open  source  via GitHub  using  HTML,  CSS,  JavaScript,  Tailwind  CSS,  Node.js,  MongoDB  \n",
      "AWS  DeepRacer  Contributor  | Amazon  Web  Service  DeepRacer  August’22  \n",
      "● Participated  in AWS  Deepracer  Student  league  and came  in the top 10% of racers  out of 4000 students  \n",
      "● Made  a Reward function  for training DeepRacer  Car on Reinforcement  Learning Model  using  AWS  ...\n",
      "------------------------\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "# These lines access the 'ResumeDatabase' and its 'Resume' collection\n",
    "db = client['ResumeDatabase']\n",
    "#\n",
    "resumes_collection = db['Resume']\n",
    "#Retrieves a specific collection named 'Resume' from the 'ResumeDatabase'.\n",
    "\n",
    "\n",
    "# In mongodb atlas, project\n",
    "# cluster\n",
    "# configuratuion\n",
    "\n",
    "# Fetch only the latest document from 'Resume' collection with sorted by \"uploadedAt\" field in decending order(-1)\n",
    "latest_resume = resumes_collection.find_one(sort=[('uploadedAt', -1)])\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "if latest_resume:  # this checks if a resume was found in the database\n",
    "    print(\"Found the latest resume in the database.\")\n",
    "\n",
    "    filename = latest_resume.get('filename', 'Unknown')\n",
    "    #provides file name if not displays \"UNKNOWN\"\n",
    "    content = latest_resume.get('content')\n",
    "    #extract the content of document\n",
    "    content_type = latest_resume.get('contentType')\n",
    "    #content type give info about nature and format of document  like PDF\"application/pdf\n",
    "#or .doc and .docx\n",
    "\n",
    "    print(f\"Processing: {filename}\") #This prints the filename of the resume being processed.\n",
    "\n",
    "\n",
    "    if content_type == 'application/pdf':\n",
    "      #this check if a document is a pdf\n",
    "        # Process PDF\n",
    "        try:\n",
    "            import io\n",
    "            from PyPDF2 import PdfReader\n",
    "\n",
    "            pdf_file = io.BytesIO(content) #These lines create a file-like object from the content and initialize a PDF reader.\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "\n",
    "             #This extracts text from each page of the PDF and concatenates it.\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "            print(f\"Extracted text from PDF (Resume Text): {text[:]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {filename}: {str(e)}\")\n",
    "\n",
    "    elif content_type in ['application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:\n",
    "        # This checks if the document is a Word file (.doc or .docx).\n",
    "\n",
    "        try:\n",
    "            import io\n",
    "            from docx import Document\n",
    "\n",
    "            #These lines create a file-like object from the content and load it as a Word document.\n",
    "            doc_file = io.BytesIO(content)\n",
    "            document = Document(doc_file)\n",
    "\n",
    "            #This extracts text from each paragraph in the Word document and joins them with newlines.\n",
    "            text = \"\\n\".join([para.text for para in document.paragraphs])\n",
    "\n",
    "            print(f\"Extracted text from Word document (Resume Text): {text[:]}...\") # this prints the extracted text frm the word document\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Word document {filename}: {str(e)}\") #this handles cases where the file type is neither PDF nor word\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {content_type}\")\n",
    "\n",
    "    print(\"------------------------\")\n",
    "\n",
    "else:\n",
    "    print(\"No resumes found in the database.\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-3mbT6fvs9r"
   },
   "source": [
    "# **Providing the resume to openai api**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:46.012081Z",
     "iopub.status.busy": "2025-02-02T03:33:46.011733Z",
     "iopub.status.idle": "2025-02-02T03:33:46.964702Z",
     "shell.execute_reply": "2025-02-02T03:33:46.963980Z"
    },
    "id": "pxdEn1CIWcmK",
    "outputId": "f3ee1df5-cf31-47d3-e423-d8a9d8849603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (1.61.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.8.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (0.8.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (2.10.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from openai) (4.12.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Requirement already satisfied: certifi in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:46.967033Z",
     "iopub.status.busy": "2025-02-02T03:33:46.966606Z",
     "iopub.status.idle": "2025-02-02T03:33:52.700062Z",
     "shell.execute_reply": "2025-02-02T03:33:52.699381Z"
    },
    "id": "4dLhEkLRXcBC",
    "outputId": "c6843226-7883-4df9-f678-7c20fff0d0b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume analysis completed and stored in database\n",
      "\n",
      "Analysis Result:\n",
      "==================================================\n",
      "Kaxit Pandya is a highly skilled Computer Engineering graduate with a Master's degree from the University of Waterloo, specializing in AI. With a strong academic background, including a Bachelor's degree from Pandit Deendayal Energy University in India, Kaxit has gained extensive experience in AI and ML as an AI/ML Data Engineer at Genellipse Inc. in Ontario, where he developed text extraction pipelines and implemented AI models for data interpretation. Kaxit's research experience spans various prestigious institutions, such as the Indian Institute of Technology and the Indian Space Research Organisation, where he worked on projects involving Deep Learning, Generative Adversarial Networks, and Federated Learning. Proficient in languages like Python, C, C++, and JavaScript, Kaxit also has expertise in libraries like TensorFlow and AWS SageMaker, along with frameworks such as Node.js and Express. His project portfolio showcases his skills in AI-powered applications, chatbots, summarization techniques, and predictive modeling. Kaxit's leadership roles in extracurricular activities, such as being an Engineering Representative and Technical Project Manager, demonstrate his ability to manage teams and deliver quality results. With certifications in LangChain Mastery, DeepLearning.AI, and Udemy courses, Kaxit is a versatile professional with a strong foundation in AI, ML, and software development, making him a valuable asset in the tech industry.\n",
      "\n",
      "Sections to Highlight:\n",
      "==================================================\n",
      "Text: Master of Engineering in Computer Engineering (Specialization AI)\n",
      "Reason: Lacks specific details and achievements\n",
      "Suggestion: Consider adding specific projects, research, or accomplishments related to the specialization in AI to showcase expertise and skills gained during the program.\n",
      "------------------------------\n",
      "Text: Coursework: Machine Learning, Artificial Intelligence, Computer Vision, Data Mining, Big Data Analytics, Data Structures, Design and Analysis of Algorithms, Web Development, Operating Systems, Software Engineering, and Database Management System\n",
      "Reason: List format can be improved for better readability\n",
      "Suggestion: Consider presenting the coursework in a bullet-point format for better readability and emphasis on each subject.\n",
      "------------------------------\n",
      "Text: Languages: Python, C, C++, JavaScript, SQL\n",
      "Reason: Lacks mention of proficiency level or experience in each language\n",
      "Suggestion: Consider adding a proficiency level or years of experience for each language to provide a clearer picture of your language skills.\n",
      "------------------------------\n",
      "Text: Area of Interest: Artificial Intelligence (AI), Machine Learning (ML), Generative AI (GenAI)\n",
      "Reason: Generic statement without specific examples or projects\n",
      "Suggestion: Consider adding specific projects, research, or experiences related to AI, ML, or GenAI to demonstrate practical application and interest in these areas.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#The OpenAI class is used to interact with OpenAI API\n",
    "import json\n",
    "#The datetime class is used for timestamping\n",
    "from datetime import datetime\n",
    "\n",
    "#This line takes two parameters:the resume text and an API key for OpenAI.\n",
    "def analyze_resume_with_openai(text, api_key):\n",
    "  #This creates an instance of the OpenAI client using the provided API key.\n",
    "  client = OpenAI(api_key=api_key)\n",
    "\n",
    "#This block sends a request to the OpenAI API to analyze the resume using GTP-3.5 turbo\n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                # \"role\": \"system\",\n",
    "                # \"content\": '''You are a resume analyzer. Analyze the following resume and provide key insights, improvements and recomendations. Do not include any concluding statements like 'Overall,' 'In conclusion,' or\n",
    "                #     'By implementing these improvements'. Focus only on presenting insights, improvements, and specific recommendations.'''\n",
    "        \n",
    "                \"role\": \"system\", \n",
    "                \"content\": '''You are a resume analyzer. Analyze the following resume and provide a summary in one paragraph under 500 words. The summary should highlight the individual's qualifications, experience, and key strengths, and explicitly make all skills the bold( write them inside **). Give it without any title'''\n",
    "                # \"content\": '''You are a resume analyzer. Analyze the following resume and provide Summary under 500 words in one paragraph also make the skills bold.'''\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        # this controls the randomness of the model output , higher temperature means increase creativity and variability\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "      # this returns the analysis result or an error message if an exception occurs\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "  except Exception as e:\n",
    "      return f\"Error analyzing resume: {str(e)}\"\n",
    "\n",
    "\n",
    "# NEW Highlight code\n",
    "def get_highlight_sections(analysis_text, api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"You are a resume improvement expert. Analyze the resume sections that need improvement and provide specific suggestions for enhancement. Your response MUST be a valid JSON string in this format:\n",
    "{\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"text\": \"exact texts from resume that needs improvement\",\n",
    "            \"reason\": \"specific reason why this needs improvement\",\n",
    "            \"suggestion\": \"detailed suggestion for how to rewrite this section\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "Do not include anything outside of the JSON structure.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Identify sections needing improvement and provide specific suggestions for enhancement: {analysis_text}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            highlights = json.loads(content)\n",
    "            if not isinstance(highlights, dict) or 'sections' not in highlights:\n",
    "                highlights = {\"sections\": []}\n",
    "            return highlights\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error parsing JSON response\")\n",
    "            return {\"sections\": []}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_highlight_sections: {str(e)}\")\n",
    "        return {\"sections\": []}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commented out bcz we used this method earlier\n",
    "# # Connect to MongoDB and fetch resume\n",
    "# db = client['ResumeDatabase']\n",
    "# resumes_collection = db['Resume']\n",
    "\n",
    "# # Fetch latest resume\n",
    "# latest_resume = resumes_collection.find_one(sort=[('uploadedAt', -1)])\n",
    "\n",
    "if latest_resume:# this line checks if the resume was found in the database\n",
    "  content = latest_resume.get('content')\n",
    "  #these lines extract the content and content type of the resume from the databse document\n",
    "  content_type = latest_resume.get('contentType')\n",
    "\n",
    "  # Extract text based on file type\n",
    "  #This block handles PDF files. It uses PyPDF2 to read the PDF content and extract text from each page.\n",
    "  text = \"\"\n",
    "  if content_type == 'application/pdf':\n",
    "      try:\n",
    "          import io\n",
    "          from PyPDF2 import PdfReader\n",
    "\n",
    "          pdf_file = io.BytesIO(content)\n",
    "          pdf_reader = PdfReader(pdf_file)\n",
    "          for page in pdf_reader.pages:\n",
    "              text += page.extract_text()\n",
    "      except Exception as e:\n",
    "          print(f\"Error processing PDF: {str(e)}\")\n",
    "\n",
    "#This block handles Word documents (.doc and .docx). It uses the python-docx library to read the document and extract text from each paragraph.\n",
    "  elif content_type in ['application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:\n",
    "      try:\n",
    "          import io\n",
    "          from docx import Document\n",
    "\n",
    "          doc_file = io.BytesIO(content)\n",
    "          document = Document(doc_file)\n",
    "          text = \"\\n\".join([para.text for para in document.paragraphs])\n",
    "      except Exception as e:\n",
    "          print(f\"Error processing Word document: {str(e)}\")\n",
    "\n",
    "  if text:# if text was successfully extracted\n",
    "      # Send to OpenAI for analysis\n",
    "      analysis = analyze_resume_with_openai(text, api_key)\n",
    "      highlights = get_highlight_sections(text, api_key)\n",
    "\n",
    "\n",
    "      # Store the analysis back in MongoDB\n",
    "      #By storing resume in database it can be accessed later without re-analyze the resume, this is cheap option also bcz user does not have to pay extra for analyzed the resume as it is stored in the database\n",
    "      # this updates the mongodb document with analysis result and current timestamp\n",
    "      resumes_collection.update_one(\n",
    "          {'_id': latest_resume['_id']},\n",
    "          {\n",
    "              '$set': {\n",
    "                  'analysis': analysis,\n",
    "                  'highlights': highlights,\n",
    "                  'analyzed_at': datetime.utcnow()\n",
    "              }\n",
    "          }\n",
    "      )\n",
    "\n",
    "        #this lines print status messages based on the output of the process\n",
    "      print(\"Resume analysis completed and stored in database\")\n",
    "  else:\n",
    "      print(\"No text could be extracted from the resume\")\n",
    "else:\n",
    "  print(\"No resume found in database\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(\"\\nAnalysis Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(analysis)\n",
    "# Modify the verification print section\n",
    "print(\"\\nSections to Highlight:\")\n",
    "print(\"=\" * 50)\n",
    "if isinstance(highlights, dict) and 'sections' in highlights:\n",
    "    for section in highlights['sections']:\n",
    "        print(f\"Text: {section['text']}\")\n",
    "        print(f\"Reason: {section['reason']}\")\n",
    "        print(f\"Suggestion: {section['suggestion']}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:52.702261Z",
     "iopub.status.busy": "2025-02-02T03:33:52.701874Z",
     "iopub.status.idle": "2025-02-02T03:33:52.705921Z",
     "shell.execute_reply": "2025-02-02T03:33:52.705399Z"
    },
    "id": "EnJDBsz8uoee",
    "outputId": "8cad201b-ce5c-4b53-b5ab-e691a15585f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API Analysis Result:\n",
      "==================================================\n",
      "Kaxit Pandya is a highly skilled Computer Engineering graduate with a Master's degree from the University of Waterloo, specializing in AI. With a strong academic background, including a Bachelor's degree from Pandit Deendayal Energy University in India, Kaxit has gained extensive experience in AI and ML as an AI/ML Data Engineer at Genellipse Inc. in Ontario, where he developed text extraction pipelines and implemented AI models for data interpretation. Kaxit's research experience spans various prestigious institutions, such as the Indian Institute of Technology and the Indian Space Research Organisation, where he worked on projects involving Deep Learning, Generative Adversarial Networks, and Federated Learning. Proficient in languages like Python, C, C++, and JavaScript, Kaxit also has expertise in libraries like TensorFlow and AWS SageMaker, along with frameworks such as Node.js and Express. His project portfolio showcases his skills in AI-powered applications, chatbots, summarization techniques, and predictive modeling. Kaxit's leadership roles in extracurricular activities, such as being an Engineering Representative and Technical Project Manager, demonstrate his ability to manage teams and deliver quality results. With certifications in LangChain Mastery, DeepLearning.AI, and Udemy courses, Kaxit is a versatile professional with a strong foundation in AI, ML, and software development, making him a valuable asset in the tech industry.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # Print the analysis result\n",
    "  #This code block is responsible for printing the results of the resume analysis\n",
    "  print(\"\\nAPI Analysis Result:\")\n",
    "  print(\"=\" * 50)\n",
    "  print(analysis)\n",
    "  print(\"=\" * 50)\n",
    "\n",
    "#any errors that occurred during the process\n",
    "except Exception as e:\n",
    "  error_message = f\"Error analyzing resume: {str(e)}\"\n",
    "  print(\"\\nError:\")\n",
    "  print(\"=\" * 50)\n",
    "  print(error_message)\n",
    "  print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkm-JXSN5Dv3"
   },
   "source": [
    "# **Scores based on orgpath pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:52.707955Z",
     "iopub.status.busy": "2025-02-02T03:33:52.707589Z",
     "iopub.status.idle": "2025-02-02T03:33:53.449816Z",
     "shell.execute_reply": "2025-02-02T03:33:53.449232Z"
    },
    "id": "CxPhWfSVHwBk",
    "outputId": "7889fe1c-40c9-4405-db8a-ee3d1608523f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API response: ```json\n",
      "{\n",
      "    \"OVERALL SCORE\": 85,\n",
      "    \"FORMATTING & PRESENTATION\": 90,\n",
      "    \"SOFT-SKILLS INFORMATION\": 80,\n",
      "    \"RELEVANT EXPERIENCE\": 90,\n",
      "    \"ACHIEVEMENT & IMPACT\": 85\n",
      "}\n",
      "```\n",
      "Resume analysis and assessment scores completed and stored in database\n",
      "\n",
      "API Analysis Result:\n",
      "==================================================\n",
      "Kaxit Pandya is a highly skilled Computer Engineering graduate with a Master's degree from the University of Waterloo, specializing in AI. With a strong academic background, including a Bachelor's degree from Pandit Deendayal Energy University in India, Kaxit has gained extensive experience in AI and ML as an AI/ML Data Engineer at Genellipse Inc. in Ontario, where he developed text extraction pipelines and implemented AI models for data interpretation. Kaxit's research experience spans various prestigious institutions, such as the Indian Institute of Technology and the Indian Space Research Organisation, where he worked on projects involving Deep Learning, Generative Adversarial Networks, and Federated Learning. Proficient in languages like Python, C, C++, and JavaScript, Kaxit also has expertise in libraries like TensorFlow and AWS SageMaker, along with frameworks such as Node.js and Express. His project portfolio showcases his skills in AI-powered applications, chatbots, summarization techniques, and predictive modeling. Kaxit's leadership roles in extracurricular activities, such as being an Engineering Representative and Technical Project Manager, demonstrate his ability to manage teams and deliver quality results. With certifications in LangChain Mastery, DeepLearning.AI, and Udemy courses, Kaxit is a versatile professional with a strong foundation in AI, ML, and software development, making him a valuable asset in the tech industry.\n",
      "\n",
      "Assessment Scores:\n",
      "==================================================\n",
      "OVERALL SCORE: 85\n",
      "FORMATTING & PRESENTATION: 90\n",
      "SOFT-SKILLS INFORMATION: 80\n",
      "RELEVANT EXPERIENCE: 90\n",
      "ACHIEVEMENT & IMPACT: 85\n",
      "==================================================\n",
      "\n",
      "Score Interpretation:\n",
      "==================================================\n",
      "OVERALL SCORE: Excellent for entry-level\n",
      "FORMATTING & PRESENTATION: Excellent for entry-level\n",
      "SOFT-SKILLS INFORMATION: Excellent for entry-level\n",
      "RELEVANT EXPERIENCE: Excellent for entry-level\n",
      "ACHIEVEMENT & IMPACT: Excellent for entry-level\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Updated criteria based on the OrgInsights Assessment guide\n",
    "    # \"Overall Score\",\n",
    "    # \"Develops Talent\",\n",
    "    # \"Builds Relationships\",\n",
    "    # \"Communicates Effectively\",\n",
    "    # \"Drives Results\",\n",
    "    # \"Demonstrates Adaptability\",\n",
    "    # \"Shows Business Acumen\",\n",
    "    # \"Thinks Strategically\",\n",
    "    # \"Leads Change\",\n",
    "    # \"Demonstrates Integrity\"\n",
    "criteria = [\n",
    "    \"OVERALL SCORE\",\n",
    "    \"FORMATTING & PRESENTATION\",\n",
    "    \"SOFT-SKILLS INFORMATION\",\n",
    "    \"RELEVANT EXPERIENCE\",\n",
    "    \"ACHIEVEMENT & IMPACT\"\n",
    "\n",
    "]\n",
    "def generate_assessment_scores(analysis_text, criteria):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that generates numerical scores for resume criteria based on the OrgInsights Assessment guide. Output should be a JSON object with scores between 0 and 100 for each criterion. \"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Based on this resume text, generate scores for the following criteria: {', '.join(criteria)}. Analysis: {analysis_text}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(\"Raw API response:\", content)  # Print raw response for debugging\n",
    "        raw_response = content.strip()\n",
    "        if raw_response.startswith(\"```json\") and raw_response.endswith(\"```\"):\n",
    "            content = raw_response[7:-3].strip()\n",
    "\n",
    "        \n",
    "\n",
    "        if content.startswith(\"``````\"):\n",
    "            content = content[3:-3]\n",
    "        if content.startswith(\"json\"):\n",
    "            content = content[4:]\n",
    "        content = content.strip()\n",
    "\n",
    "        scores = json.loads(content)\n",
    "        return scores\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {str(e)}\")\n",
    "        print(\"Raw content causing error:\", content)\n",
    "\n",
    "        return f\"Error parsing JSON: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"General Error: {str(e)}\")\n",
    "        return f\"Error generating assessment scores: {str(e)}\"\n",
    "\n",
    "# Assuming you have the analysis text stored in a variable called 'analysis'\n",
    "scores = generate_assessment_scores(analysis, criteria)\n",
    "\n",
    "if isinstance(scores, dict):\n",
    "    # Store the analysis and scores back in MongoDB\n",
    "    resumes_collection.update_one(\n",
    "        {'_id': latest_resume['_id']},\n",
    "        {\n",
    "            '$set': {\n",
    "                'assessment_scores': scores,\n",
    "                'scored_at': datetime.utcnow()\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Resume analysis and assessment scores completed and stored in database\")\n",
    "\n",
    "    print(\"\\nAPI Analysis Result:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(analysis)\n",
    "    print(\"\\nAssessment Scores:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for criterion, score in scores.items():\n",
    "        print(f\"{criterion}: {score}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Add interpretation of scores based on the guide\n",
    "    print(\"\\nScore Interpretation:\")\n",
    "    print(\"=\" * 50)\n",
    "    experience_level = \"Entry-level\"  # You may need to determine this based on the resume content\n",
    "    for criterion, score in scores.items():\n",
    "        if criterion != \"Overall Score\":\n",
    "            if experience_level == \"Entry-level\":\n",
    "                if score >= 70:\n",
    "                    print(f\"{criterion}: Excellent for entry-level\")\n",
    "                elif score >= 50:\n",
    "                    print(f\"{criterion}: Good for entry-level\")\n",
    "                else:\n",
    "                    print(f\"{criterion}: Needs improvement\")\n",
    "            else:\n",
    "                if score >= 80:\n",
    "                    print(f\"{criterion}: Meets senior-level expectations\")\n",
    "                elif score >= 60:\n",
    "                    print(f\"{criterion}: Acceptable, but room for growth\")\n",
    "                else:\n",
    "                    print(f\"{criterion}: Below expectations for experienced professionals\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "else:\n",
    "    print(\"Error in generating scores:\")\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elevator Pitch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:53.451675Z",
     "iopub.status.busy": "2025-02-02T03:33:53.451482Z",
     "iopub.status.idle": "2025-02-02T03:33:54.861530Z",
     "shell.execute_reply": "2025-02-02T03:33:54.860979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevator pitch generated and stored successfully:\n",
      "Hello, I'm Kaxit Pandya, a Master of Engineering in Computer Engineering with a specialization in AI. With a strong background in AI and ML, I've developed text extraction pipelines, implemented AI models for data interpretation, and conducted research using GANs for material exploration. My experience includes web scraping, image processing, and predictive modeling for water quality parameters. I've also contributed to projects like an Instagram Caption Generator and a DeepFake Audio Detection System. As a technical project manager at the Waterloo AI Institute, I lead AI research projects and collaborate with industry partners. With a proven track record of innovation and leadership, I bring a unique blend of technical expertise and project management skills to drive impactful solutions in the AI domain.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def generate_elevator_pitch(resume_text, api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that creates concise and impactful elevator pitches based on resume information. The elevator pitch should be no more than 1 minute long when spoken, highlighting the candidate's key strengths, experiences, and unique value proposition.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Based on the following resume, create an elevator pitch for him/her, write in 1st person tone:\\n\\n{resume_text}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        elevator_pitch = response.choices[0].message.content.strip()\n",
    "        return elevator_pitch\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating elevator pitch: {str(e)}\"\n",
    "\n",
    "\n",
    "resume_text = text\n",
    "\n",
    "# Generate the elevator pitch\n",
    "try:\n",
    "    elevator_pitch = generate_elevator_pitch(resume_text, api_key)\n",
    "\n",
    "    # Store the elevator pitch in MongoDB (assuming you have a MongoDB connection set up)\n",
    "    resumes_collection.update_one(\n",
    "        {'_id': latest_resume['_id']},\n",
    "        {\n",
    "            '$set': {\n",
    "                'elevator_pitch': elevator_pitch,\n",
    "                'pitch_generated_at': datetime.now(timezone.utc)\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Elevator pitch generated and stored successfully:\")\n",
    "    print(elevator_pitch)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:54.863494Z",
     "iopub.status.busy": "2025-02-02T03:33:54.863288Z",
     "iopub.status.idle": "2025-02-02T03:33:54.869948Z",
     "shell.execute_reply": "2025-02-02T03:33:54.869456Z"
    }
   },
   "outputs": [],
   "source": [
    "star_text='''Pre-Interview Preparation: What Every Job Seeker And New Graduate Should Do Before The First Phone Screen Or Interview\n",
    "Before you even get on a phone call or step into an interview room, proper preparation is key to ensuring you make a strong impres-sion. Taking the time to prepare not only helps you feel more confi-dent, but it also shows potential employers that you are serious about the opportunity.\n",
    "Research the Company Thoroughly\n",
    "Before any interaction with the hiring team, it's critical to research the company. A lack of knowledge about the employer can hurt your chances, as it suggests you’re uninterested or unprepared. Here’s what you need to focus on:\n",
    "• Company Overview: Familiarize yourself with the company’s mission, values, and culture. Check their website, social media channels, and any news articles to stay informed about recent developments.\n",
    "• Industry: Understand the company’s position in its industry and its competitors. You don’t need to be an expert, but having general knowledge shows you’re serious about the opportunity.\n",
    "• Key Players: Look up the leadership team, especially anyone who might be interviewing you. Knowing who you’ll be speaking with can give you an edge and help you tailor your questions or conversation.\n",
    "Study the Job Description in Detail\n",
    "A well-prepared candidate knows the job description inside and out. Break down the requirements and make sure you understand what’s expected of the role. Match your skills, experience, and achievements to the key qualifications they are looking for.\n",
    "• Identify your strengths: Highlight areas where your skills perfectly align with the job description.\n",
    "• Prepare to address any gaps: If you don’t meet 100% of the qualifications, be ready to discuss how your other skills or your ability to learn quickly can bridge those gaps.\n",
    "Prepare Your Key Talking Points\n",
    "You’ll want to walk into the interview ready to discuss your background, but it’s important to focus on what’s relevant to the position. Here’s how to prepare:\n",
    "• Craft your elevator pitch: Prepare a concise summary of who you are, your professional background, and what you bring to the role.\n",
    "• Frame your experience: Think about 3-5 accomplishments or projects that best demonstrate your abilities and tie them directly to the job you’re applying for.\n",
    "LEVELLING UP YOUR INTERVIEW GAME:\n",
    "BECOMING A STAR\n",
    "15\n",
    "Pre-Interview Preparation: What Every Job Seeker And New Graduate Should Do Before The First Phone Screen Or Interview Cont...\n",
    "Prepare Thoughtful Questions\n",
    "Hiring managers are evaluating you, but you should also be evaluating the company. Prepare questions that not only demonstrate your interest in the role but also help you gauge whether the company is the right fit for you. Examples include:\n",
    "“What are the key goals for someone in this role in the first 6-12 months?”\n",
    "“Can you tell me about the company culture and team dynamics?”\n",
    "“What does success look like in this position?”\n",
    "Asking thoughtful questions can show your genuine interest in the role, while also giving you deeper in-sight into the position and the company.\n",
    "Test Your Technology for Virtual Interviews\n",
    "If your interview is going to be virtual or over the phone, don’t wait until the last minute to check your equipment. Technical issues can quickly derail a great conversation and leave a poor impression.\n",
    "• Test your internet connection: Make sure it’s stable and reliable.\n",
    "• Test your camera and microphone: For video interviews, ensure the camera is positioned well, with good lighting, and that your microphone is clear.\n",
    "• Find a quiet space: Minimize background noise, and ensure that your surroundings are professional and free from distractions.\n",
    "Review Your Resume and LinkedIn Profile\n",
    "Make sure your resume is up-to-date and consistent with what’s on your LinkedIn profile. Hiring managers often check both. Be prepared to talk through any gaps in employment, transitions between jobs, or spe-cific projects you’ve highlighted. Also, make sure your LinkedIn profile is polished and reflects the same key points as your resume.\n",
    "Practice Mock Interviews\n",
    "Whether it’s with a friend, mentor, or even in front of a mirror, mock interviews help you polish your re-sponses and get comfortable speaking about your experience. Focus on:\n",
    "• Clear, concise answers: Don’t ramble or go off-topic.\n",
    "• Body language: Make sure to maintain eye contact, sit up straight, and use appropriate gestures.\n",
    "• Pacing and tone: Avoid speaking too fast or too slowly. Practice maintaining an upbeat, confident tone.\n",
    "Know Your Availability and Salary Expectations\n",
    "Be prepared to discuss your availability for the role, including your potential start date. If salary is likely to come up, have a range ready based on your research of the market and your level of experience. Sites like Glassdoor and PayScale can help you find industry-standard salaries for similar positions.\n",
    "Get a Good Night’s Sleep\n",
    "It might seem obvious, but getting enough sleep the night before is essential for being at your best. A well-rested mind will help you stay calm, focused, and responsive during your interview.\n",
    "16\n",
    "Dress For Success - Interview Attire and Etiquette\n",
    "When it comes to interviews, both in-person and virtual, the way you present yourself can make a lasting impression. Whether you're a seasoned professional or a fresh graduate, dressing and be-having appropriately plays a crucial role in shaping how employers perceive you.\n",
    "In-Person Interviews:\n",
    "• Understand the Company Culture: Research the company's dress code beforehand. If it's a cor-porate or conservative environment, opt for formal attire (e.g., suits or professional dresses). For more relaxed workplaces, business casual is acceptable, but it's better to be slightly overdressed than underdressed.\n",
    "• Invest in Fit and Quality: Your clothing should be clean, well-fitted, and wrinkle-free. Ill-fitting clothes can be distracting and suggest a lack of attention to detail. Polish your shoes and avoid flashy or excessive accessories.\n",
    "• Neutral and Professional Colors: Stick to neutral colors like navy, black, gray, or white. These con-vey professionalism and are less likely to distract the interviewer from your qualifications and skills.\n",
    "• Grooming Matters: Personal hygiene is paramount. Make sure your hair is neat, your nails are clean, and your breath is fresh. Avoid overpowering colognes or perfumes, as these can be dis-tracting or bothersome in a small room.\n",
    "Online Interviews:\n",
    "• Professional from the Waist Up: Even if you're behind a screen, dress as if you’re meeting in per-son. At the very least, wear a button-up shirt, blouse, or blazer. Avoid the temptation to wear sweatpants or pajamas below the camera line—you never know when you'll need to stand up!\n",
    "• Choose a Simple Background: Your interview setting should be distraction-free. A plain wall or a tidy space will keep the focus on you, not your surroundings. Ensure proper lighting, and sit facing a window or a light source for a clear, bright appearance.\n",
    "• Test Technology in Advance: Dress rehearsal isn't just for actors—test your equipment (camera, mic, internet) before the interview to avoid any last-minute hiccups. It shows you're prepared and serious about the opportunity.\n",
    "17\n",
    "How To Highlight Soft Skills in An Interview\n",
    "The interview is the time to show off those soft skills in action. Here’s how you can subtly weave them in-to your answers:\n",
    "Use the STAR Method to Show Problem-Solving & Teamwork\n",
    "The STAR method (Situation, Task, Action, Result) is perfect for soft skills questions. It gives you a struc-ture for storytelling that really highlights how you’ve used your skills in real life.\n",
    "Example Question: “Tell me about a time you worked in a team.”\n",
    "Answer using STAR:\n",
    "• Situation: “At my internship, we had to work on a project with a tight deadline.”\n",
    "• Task: “I was responsible for coordinating between the marketing and sales teams to ensure every-one was on the same page.”\n",
    "• Action: “I set up weekly meetings and clear communication channels to keep everyone aligned.”\n",
    "Result: “We finished the project ahead of schedule, and our cross-team collaboration increased by 25%.”\n",
    "This shows teamwork, communication, and time management—all in one story! Plus, it paints you as a capable team player with real results.\n",
    "When They Ask About Strengths, Highlight Soft Skills\n",
    "When the interviewer says, “Tell me about your strengths,” it’s your time to shine. Don’t just say you’re a “good communicator”—talk about how you communicate well.\n",
    "Example Answer: “I think one of my biggest strengths is my ability to communicate with people from different backgrounds. For example, at my last job, I regularly had to explain technical information to non-technical team members, and I got really good at simplifying complex concepts so everyone understood.”\n",
    "This way, you’re proving the strength instead of just saying it.\n",
    "Answer Behavioral Questions with Soft Skills in Mind\n",
    "Behavioral questions are designed to find out how you’ll handle situations at work. Always answer these with soft skills at the forefront.\n",
    "Example Behavioral Question: “How do you handle stress or tight deadlines?”\n",
    "Answer: “I prioritize tasks and focus on what’s most urgent first. For example, during finals week at school, I had to juggle multiple assignments. I broke them down into smaller steps and communicated with my professors about expectations. By staying organized and calm, I was able to meet all my dead-lines without sacrificing quality.”\n",
    "You’ve just demonstrated adaptability, time management, and communication in one answer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T03:33:54.871937Z",
     "iopub.status.busy": "2025-02-02T03:33:54.871491Z",
     "iopub.status.idle": "2025-02-02T03:33:59.074650Z",
     "shell.execute_reply": "2025-02-02T03:33:59.073972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAR INFO generated and stored successfully:\n",
      "**Situation**: Developed Deep Neural Networks for predicting classification of Pulse Radar Interval Modulation at Indian Space Research Organisation (ISRO), SAC, Ahmedabad, Gujarat.\n",
      "\n",
      "**Task**: Processed and handled noisy, raw radar pulse data directly from ISRO’s satellites.\n",
      "\n",
      "**Action**: Achieved 94% classification accuracy in identifying complex radar Pulse Repetition Intervals (PRI) patterns, including staggered, jittered, dwell and switch, sliding, linear, and sinusoidal.\n",
      "\n",
      "**Result**: Successfully implemented Deep Neural Networks for accurate classification of Pulse Radar Interval Modulation, showcasing expertise in radar signal processing and machine learning.\n",
      "\n",
      "---\n",
      "\n",
      "**Situation**: Implemented Federated Learning (FL) model to predict water quality parameters using drone-captured images at TEEP@ASIA PLUS PROGRAM, National Chung Cheng University, Chiayi County, Taiwan.\n",
      "\n",
      "**Task**: Ensured privacy across geographically distributed datasets while predicting water quality parameters.\n",
      "\n",
      "**Action**: Executed robust aggregation strategies such as FedAvg, FedAvgM, FedMedian, and FedProx using the Flower Framework, incorporating Explainable AI (XAI) techniques like SHAP to interpret predictions.\n",
      "\n",
      "**Result**: Successfully predicted water quality parameters with high accuracy using Federated Learning, demonstrating expertise in privacy-preserving machine learning techniques and model interpretability.\n",
      "\n",
      "---\n",
      "\n",
      "**Situation**: Built a text extraction pipeline for processing diverse document formats at Genellipse Inc., Mississauga, Ontario.\n",
      "\n",
      "**Task**: Leveraged Google Cloud Vision API to extract data from U.S. death certificates with high accuracy.\n",
      "\n",
      "**Action**: Utilized BERT-based AI models to map key-value pairs from U.S. death certificate fields accurately, delivering actionable insights for enhanced client reporting and decision-making.\n",
      "\n",
      "**Result**: Enhanced data extraction efficiency and accuracy for U.S. death certificates, showcasing proficiency in AI-driven document processing and analysis.\n"
     ]
    }
   ],
   "source": [
    "def generate_star(star_text, text):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f'''Fetch information about STAR method, This is some star information:\\n\\n {star_text} \\n\\n You are a STAR (Situation, Task, Action, Result) master. You give star information by reviewing their resume. Give result without any starting or ending statements, just in different points. Also dont add any \"-\" in between. Format the output so that the words **Situation**, **Task**, **Action**, and **Result** are bold using markdown syntax .'''\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"This is the resume text:\\n\\n {text} \\n\\n analyse it and identify specific situations, tasks, actions, and results from it that can be used during an interview. Clearly label each STAR component and provide 3 STAR examples:\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7                                                                   \n",
    "        )\n",
    "\n",
    "        star_info = response.choices[0].message.content.strip()\n",
    "        return star_info\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating elevator pitch: {str(e)}\"\n",
    "\n",
    "\n",
    "try:\n",
    "    star_info = generate_star(star_text, text)\n",
    "\n",
    "    # Store the elevator pitch in MongoDB (assuming you have a MongoDB connection set up)\n",
    "    resumes_collection.update_one(\n",
    "        {'_id': latest_resume['_id']},\n",
    "        {\n",
    "            '$set': {\n",
    "                'star_info': star_info,\n",
    "                'star_generated_at': datetime.now(timezone.utc)\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"STAR INFO generated and stored successfully:\")\n",
    "    print(star_info)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
